// TODO refine

// 
// the original Tokenizer generated by syntax-cli
// 
// // const
// static EOF: &'static str = "$";
//
// pub struct Tokenizer {
//     /**
//      * Tokenizing string.
//      */
//     string: &'static str,
//
//     /**
//      * Cursor for current symbol.
//      */
//     cursor: i32,
//
//     /**
//      * States.
//      */
//     states: Vec<&'static str>,
//
//     /**
//      * Line-based location tracking.
//      */
//     current_line: i32,
//     current_column: i32,
//     current_line_begin_offset: i32,
//
//     /**
//      * Location data of a matched token.
//      */
//     token_start_offset: i32,
//     token_end_offset: i32,
//     token_start_line: i32,
//     token_end_line: i32,
//     token_start_column: i32,
//     token_end_column: i32,
//
//     /**
//      * Matched text, and its length.
//      */
//     yytext: &'static str,
//     yyleng: usize,
//
//     handlers: [fn(&mut Tokenizer) -> &'static str; 0],
// }
//
// impl Tokenizer {
//
//     /**
//      * Creates a new Tokenizer instance.
//      *
//      * The same instance can be then reused in parser
//      * by calling `init_string`.
//      */
//     pub fn new() -> Tokenizer {
//         let mut tokenizer = Tokenizer {
//             string: "",
//             cursor: 0,
//
//             states: Vec::new(),
//
//             current_line: 1,
//             current_column: 0,
//             current_line_begin_offset: 0,
//
//             token_start_offset: 0,
//             token_end_offset: 0,
//             token_start_line: 0,
//             token_end_line: 0,
//             token_start_column: 0,
//             token_end_column: 0,
//
//             yytext: "",
//             yyleng: 0,
//
//             handlers: [
    
// ],
//         };
//
//         tokenizer
//     }
//
//     /**
//      * Initializes a parsing string.
//      */
//     pub fn init_string(&mut self, string: &'static str) -> &mut Tokenizer {
//         self.string = string;
//
//         // Initialize states.
//         self.states.clear();
//         self.states.push("INITIAL");
//
//         self.cursor = 0;
//         self.current_line = 1;
//         self.current_column = 0;
//         self.current_line_begin_offset = 0;
//
//         self.token_start_offset = 0;
//         self.token_end_offset = 0;
//         self.token_start_line = 0;
//         self.token_end_line = 0;
//         self.token_start_column = 0;
//         self.token_end_column = 0;
//
//         self
//     }
//
//     /**
//      * Returns next token.
//      */
//     pub fn get_next_token(&mut self) -> Token {
//         if !self.has_more_tokens() {
//             self.yytext = EOF;
//             return self.to_token(EOF)
//         }
//
//         let str_slice = &self.string[self.cursor as usize..];
//
//         let lex_rules_for_state = LEX_RULES_BY_START_CONDITIONS
//             .get(self.get_current_state())
//             .unwrap();
//
//         for i in 0..lex_rules_for_state.len() {
//             let lex_rule = LEX_RULES[i];
//
//             if let Some(matched) = self._match(str_slice, &Regex::new(lex_rule).unwrap()) {
//
//                 // Manual handling of EOF token (the end of string). Return it
//                 // as `EOF` symbol.
//                 if matched.len() == 0 {
//                     self.cursor = self.cursor + 1;
//                 }
//
//                 self.yytext = matched;
//                 self.yyleng = matched.len();
//
//                 let token_type = self.handlers[i](self);
//
//                 // "" - no token (skip)
//                 if token_type.len() == 0 {
//                     return self.get_next_token();
//                 }
//
//                 return self.to_token(token_type)
//             }
//         }
//
//         if self.is_eof() {
//             self.cursor = self.cursor + 1;
//             self.yytext = EOF;
//             return self.to_token(EOF);
//         }
//
//         self.panic_unexpected_token(
//             &str_slice[0..1],
//             self.current_line,
//             self.current_column
//         );
//
//         unreachable!()
//     }
//
//     /**
//      * Throws default "Unexpected token" exception, showing the actual
//      * line from the source, pointing with the ^ marker to the bad token.
//      * In addition, shows `line:column` location.
//      */
//     fn panic_unexpected_token(&self, string: &'static str, line: i32, column: i32) {
//         let line_source = self.string
//             .split('\n')
//             .collect::<Vec<&str>>()
//             [(line - 1) as usize];
//
//         let pad = ::std::iter::repeat(" ")
//             .take(column as usize)
//             .collect::<String>()ÃŸ;
//
//         let line_data = format!("\n\n{}\n{}^\n", line_source, pad);
//
//         panic!(
//             "{} Unexpected token: \"{}\" at {}:{}.",
//             line_data,
//             string,
//             line,
//             column
//         );
//     }
//
//     fn capture_location(&mut self, matched: &'static str) {
//         let nl_re = Regex::new(r"\n").unwrap();
//
//         // Absolute offsets.
//         self.token_start_offset = self.cursor;
//
//         // Line-based locations, start.
//         self.token_start_line = self.current_line;
//         self.token_start_column = self.token_start_offset - self.current_line_begin_offset;
//
//         // Extract `\n` in the matched token.
//         for cap in nl_re.captures_iter(matched) {
//             self.current_line = self.current_line + 1;
//             self.current_line_begin_offset = self.token_start_offset +
//                 cap.get(0).unwrap().start() as i32 + 1;
//         }
//
//         self.token_end_offset = self.cursor + matched.len() as i32;
//
//         // Line-based locations, end.
//         self.token_end_line = self.current_line;
//         self.token_end_column = self.token_end_offset - self.current_line_begin_offset;
//         self.current_column = self.token_end_column;
//     }
//
//     fn _match(&mut self, str_slice: &'static str, re: &Regex) -> Option<&'static str> {
//         match re.captures(str_slice) {
//             Some(caps) => {
//                 let matched = caps.get(0).unwrap().as_str();
//                 self.capture_location(matched);
//                 self.cursor = self.cursor + (matched.len() as i32);
//                 Some(matched)
//             },
//             None => None
//         }
//     }
//
//     fn to_token(&self, token_type: &'static str) -> Token {
//         Token {
//             kind: *TOKENS_MAP.get(token_type).unwrap(),
//             value: self.yytext,
//             start_offset: self.token_start_offset,
//             end_offset: self.token_end_offset,
//             start_line: self.token_start_line,
//             end_line: self.token_end_line,
//             start_column: self.token_start_column,
//             end_column: self.token_end_column,
//         }
//     }
//
//     /**
//      * Whether there are still tokens in the stream.
//      */
//     pub fn has_more_tokens(&mut self) -> bool {
//         self.cursor <= self.string.len() as i32
//     }
//
//     /**
//      * Whether the cursor is at the EOF.
//      */
//     pub fn is_eof(&mut self) -> bool {
//         self.cursor == self.string.len() as i32
//     }
//
//     /**
//      * Returns current tokenizing state.
//      */
//     pub fn get_current_state(&mut self) -> &'static str {
//         match self.states.last() {
//             Some(last) => last,
//             None => "INITIAL"
//         }
//     }
//
//     /**
//      * Enters a new state pushing it on the states stack.
//      */
//     pub fn push_state(&mut self, state: &'static str) -> &mut Tokenizer {
//         self.states.push(state);
//         self
//     }
//
//     /**
//      * Alias for `push_state`.
//      */
//     pub fn begin(&mut self, state: &'static str) -> &mut Tokenizer {
//         self.push_state(state);
//         self
//     }
//
//     /**
//      * Exits a current state popping it from the states stack.
//      */
//     pub fn pop_state(&mut self) -> &'static str {
//         match self.states.pop() {
//             Some(top) => top,
//             None => "INITIAL"
//         }
//     }
//
// }

// use regex::Regex;

// use token::token::Token as InteriorToken;
use parser::token::{ Token, get_an_eof_token };

use lexer::lexer::Lexer;

pub struct Tokenizer {
    // string: &'static str,

    // TODO seems unnecessary
    pub yytext: &'static str,
    pub yyleng: usize,

    pub interior_lexer: Lexer,

    no_more_tokens: bool,
}

impl Tokenizer {
    pub fn new() -> Tokenizer {
        let mut tokenizer = Tokenizer {
            // string: "",

            yytext: "",
            yyleng: 0,

            interior_lexer: Lexer::new(String::from("")),

            no_more_tokens: false,
        };

        tokenizer
    }

    pub fn init_string(&mut self, string: &str) -> &mut Tokenizer {
        // self.string = string;
        self.interior_lexer = Lexer::new(String::from(string));

        self
    }

    pub fn get_next_token(&mut self) -> Token {
        if let Some(interior_token) = self.interior_lexer.advance() {
            let token = interior_token.wrap_as_token();

            println!("###### TOKENIZER#get_next_token: got token: {:?}", token);

            token
        } else {
            println!("###### TOKENIZER#get_next_token: no_more_tokens");
            self.no_more_tokens = true;
            get_an_eof_token()
        }

    }

    // TODO
    pub fn has_more_tokens(&mut self) -> bool {
        println!("has_more_tokens? invoking, result: {}", !self.no_more_tokens);

        !self.no_more_tokens
    }

    pub fn panic_unexpected_token(&self, string: &'static str, line: i32, column: i32) {
        panic!("unexpected_token, token: {:?}", string);
    }
}
